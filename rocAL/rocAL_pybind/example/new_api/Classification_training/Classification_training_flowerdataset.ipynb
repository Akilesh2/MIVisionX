{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c9244e",
   "metadata": {},
   "source": [
    "## Rocal Classification training \n",
    "In this notebook we will show example of rocAL classification training with small  dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73bdd89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "import time \n",
    "from amd.rocal.plugin.pytorch import ROCALClassificationIterator\n",
    "from amd.rocal.pipeline import Pipeline\n",
    "import amd.rocal.fn as fn\n",
    "import amd.rocal.types as types\n",
    "from torch.optim import Optimizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a06c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "device = torch.device( 'cpu')\n",
    "device\n",
    "\n",
    "data_dir = os.listdir(\"./Flower102/split_data/\")\n",
    "\n",
    "data_dir = './Flower102/split_data/'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/val'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0174c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28cbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.files = []\n",
    "        for (dirpath, _, filenames) in os.walk(self.path):\n",
    "            for f in filenames:\n",
    "                if f.endswith('.jpg'):\n",
    "                    p = {}\n",
    "                    p['img_path'] = dirpath + '/' + f\n",
    "                    self.files.append(p)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]['img_path']\n",
    "        img_name = img_path.split('/')[-1]\n",
    "        image = pil_loader(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0, img_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8acc2",
   "metadata": {},
   "source": [
    "## Defining the Pipeline\n",
    "We are defining a pipeline for a classification task. Our pipeline will read images from a directory, decode them and return (image, label) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90041830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(data_path, batch_size, num_classes, one_hot, local_rank, world_size, num_thread, crop, rocal_cpu, fp16):\n",
    "    pipe = Pipeline(batch_size=batch_size, num_threads=num_thread, device_id=local_rank, seed=local_rank+10, rocal_cpu=rocal_cpu, tensor_dtype = types.FLOAT16 if fp16 else types.FLOAT, tensor_layout=types.NCHW, prefetch_queue_depth = 6)\n",
    "    with pipe:\n",
    "        jpegs, labels = fn.readers.file(file_root=data_path, shard_id=local_rank, num_shards=world_size, random_shuffle=True, seed=local_rank+10)\n",
    "        rocal_device = 'cpu' if rocal_cpu else 'gpu'\n",
    "        decode = fn.decoders.image_slice(jpegs, output_type=types.RGB,\n",
    "                                                    file_root=data_path, shard_id=1, num_shards=10, random_shuffle=True, seed=local_rank+10)\n",
    "        res = fn.resize(decode, resize_x=224, resize_y=224, seed=local_rank+10,  interpolation_type=types.TRIANGULAR_INTERPOLATION)\n",
    "        cmnp = fn.crop_mirror_normalize(res, device=\"gpu\",\n",
    "                                            output_dtype=types.FLOAT,\n",
    "                                            output_layout=types.NCHW,\n",
    "                                            crop=(224, 224),\n",
    "                                            mirror=0,\n",
    "                                            image_type=types.RGB,\n",
    "                                            mean = [0,0,0],std=[1,1,1],\n",
    "                                            seed=local_rank+10)\n",
    "        if(one_hot):\n",
    "            _ = fn.one_hot(labels, num_classes)\n",
    "        pipe.set_outputs(cmnp)\n",
    "    print('rocal \"{0}\" variant'.format(rocal_device))\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60094a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_pipeline(data_path, batch_size, num_classes, one_hot, local_rank, world_size, num_thread, crop, rocal_cpu, fp16):\n",
    "    pipe = Pipeline(batch_size=batch_size, num_threads=num_thread, device_id=local_rank, seed=local_rank + 10, rocal_cpu=rocal_cpu, tensor_dtype = types.FLOAT16 if fp16 else types.FLOAT, tensor_layout=types.NCHW, prefetch_queue_depth = 2)\n",
    "    with pipe:\n",
    "        jpegs, labels = fn.readers.file(file_root=data_path, seed=local_rank+10)\n",
    "        rocal_device = 'cpu' if rocal_cpu else 'gpu'\n",
    "        decode = fn.decoders.image_slice(jpegs, output_type=types.RGB,file_root=data_path, shard_id=1, num_shards=10, random_shuffle=False, seed=local_rank+10)\n",
    "        res = fn.resize(decode, resize_x=224, resize_y=224, seed=local_rank+10,  interpolation_type=types.TRIANGULAR_INTERPOLATION)\n",
    "\n",
    "        cmnp = fn.crop_mirror_normalize(res , device=\"cpu\",\n",
    "                                            output_dtype=types.FLOAT16 if fp16 else types.FLOAT,\n",
    "                                            output_layout=types.NCHW,\n",
    "                                            crop=(224, 224),\n",
    "                                            mirror=0,\n",
    "                                            seed = local_rank+10,\n",
    "                                            image_type=types.RGB,\n",
    "                                            mean=[0,0,0],\n",
    "                                            std=[1,1,1])\n",
    "        if(one_hot):\n",
    "            _ = fn.one_hot(labels, num_classes)\n",
    "        pipe.set_outputs(cmnp)\n",
    "    print('rocal \"{0}\" variant'.format(rocal_device))\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538094db",
   "metadata": {},
   "source": [
    "## Building the Pipeline\n",
    "Here we are creating the pipeline. In order to use our Pipeline, we need to build it. This is achieved by calling the build function.\n",
    "Then iterator object is created with ROCALClassificationIterator(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838fea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline has been created succesfully\n",
      "OK: loaded 81 kernels from libvx_rpp.so\n",
      "rocal \"cpu\" variant\n"
     ]
    }
   ],
   "source": [
    "pipe = train_pipeline(data_path=train_dir, batch_size=64, num_classes=1, one_hot=0, local_rank=1 , world_size=1 , num_thread=3, crop=10, rocal_cpu='cpu', fp16=False)\n",
    "pipe.build()\n",
    "trainloader = ROCALClassificationIterator(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50d4b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline has been created succesfully\n",
      "rocal \"cpu\" variant\n",
      "OK: loaded 81 kernels from libvx_rpp.so\n"
     ]
    }
   ],
   "source": [
    "pipe = val_pipeline(data_path=valid_dir, batch_size=64, num_classes=1, one_hot=0, local_rank=1 , world_size=1 , num_thread=3, crop=10, rocal_cpu='cpu', fp16=False)\n",
    "pipe.build()\n",
    "validloader = ROCALClassificationIterator(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37e1447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, is_test=False):\n",
    "    global total\n",
    "    global correct\n",
    "    batch_size = target.size(0)\n",
    "    total += batch_size    \n",
    "    _, pred = output.max(dim=1)\n",
    "    if is_test:\n",
    "        preds.extend(pred)\n",
    "    correct += torch.sum(pred == target.data)\n",
    "    return  (correct.float()/total) * 100\n",
    "\n",
    "def reset():\n",
    "    global total, correct\n",
    "    global train_loss, test_loss, best_acc\n",
    "    global trn_losses, trn_accs, val_losses, val_accs\n",
    "    total, correct = 0, 0\n",
    "    train_loss, test_loss, best_acc = 0.0, 0.0, 0.0\n",
    "    trn_losses, trn_accs, val_losses, val_accs = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "527e3311",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgStats(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.losses =[]\n",
    "        self.precs =[]\n",
    "        self.its = []\n",
    "        \n",
    "    def append(self, loss, prec, it):\n",
    "        self.losses.append(loss)\n",
    "        self.precs.append(prec)\n",
    "        self.its.append(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fff85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, is_best, filename='./checkpoint.pth.tar'):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        torch.save(model.state_dict(), filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df0c62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, filename = './checkpoint.pth.tar'):\n",
    "    sd = torch.load(filename, map_location=lambda storage, loc: storage)\n",
    "    names = set(model.state_dict().keys())\n",
    "    for n in list(sd.keys()): \n",
    "        if n not in names and n+'_raw' in names:\n",
    "            if n+'_raw' not in sd: sd[n+'_raw'] = sd[n]\n",
    "            del sd[n]\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8448f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLR(object):\n",
    "    \"\"\"\n",
    "    The method is described in paper : https://arxiv.org/abs/1506.01186 to find out optimum \n",
    "    learning rate. The learning rate is increased from lower value to higher per iteration \n",
    "    for some iterations till loss starts exploding.The learning rate one power lower than \n",
    "    the one where loss is minimum is chosen as optimum learning rate for training.\n",
    "\n",
    "    Args:\n",
    "        optim   Optimizer used in training.\n",
    "\n",
    "        bn      Total number of iterations used for this test run.\n",
    "                The learning rate increasing factor is calculated based on this \n",
    "                iteration number.\n",
    "\n",
    "        base_lr The lower boundary for learning rate which will be used as\n",
    "                initial learning rate during test run. It is adviced to start from\n",
    "                small learning rate value like 1e-4.\n",
    "                Default value is 1e-5\n",
    "\n",
    "        max_lr  The upper boundary for learning rate. This value defines amplitude\n",
    "                for learning rate increase(max_lr-base_lr). max_lr value may not be \n",
    "                reached in test run as loss may explode before reaching max_lr.\n",
    "                It is adviced to use higher value like 10, 100.\n",
    "                Default value is 100.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, optim, bn, base_lr=1e-7, max_lr=100):\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.optim = optim\n",
    "        self.bn = bn - 1\n",
    "        ratio = self.max_lr/self.base_lr\n",
    "        self.mult = ratio ** (1/self.bn)\n",
    "        self.best_loss = 1e9\n",
    "        self.iteration = 0\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def calc_lr(self, loss):\n",
    "        self.iteration +=1\n",
    "        if math.isnan(loss) or loss > 4 * self.best_loss:\n",
    "            return -1\n",
    "        if loss < self.best_loss and self.iteration > 1:\n",
    "            self.best_loss = loss\n",
    "            \n",
    "        mult = self.mult ** self.iteration\n",
    "        lr = self.base_lr * mult\n",
    "        \n",
    "        self.lrs.append(lr)\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        return lr\n",
    "        \n",
    "    def plot(self, start=10, end=-5):\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Losses\")\n",
    "        plt.plot(self.lrs[start:end], self.losses[start:end])\n",
    "        plt.xscale('log')\n",
    "        \n",
    "        \n",
    "    def plot_lr(self):\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.plot(self.lrs)\n",
    "        plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cdb0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(Optimizer):\n",
    "    r'''Implements Lookahead optimizer.\n",
    "\n",
    "    It's been proposed in paper: Lookahead Optimizer: k steps forward, 1 step back\n",
    "    (https://arxiv.org/pdf/1907.08610.pdf)\n",
    "\n",
    "    Args:\n",
    "        optimizer: The optimizer object used in inner loop for fast weight updates.\n",
    "        alpha:     The learning rate for slow weight update.\n",
    "                   Default: 0.5\n",
    "        k:         Number of iterations of fast weights updates before updating slow\n",
    "                   weights.\n",
    "                   Default: 5\n",
    "\n",
    "    Example:\n",
    "        > optim = Lookahead(optimizer)\n",
    "        > optim = Lookahead(optimizer, alpha=0.6, k=10)\n",
    "    '''\n",
    "    def __init__(self, optimizer, alpha=0.5, k=5):\n",
    "        assert(0.0 <= alpha <= 1.0)\n",
    "        assert(k >= 1)\n",
    "        self.optimizer = optimizer\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            group['k_counter'] = 0\n",
    "        self.slow_weights = [[param.clone().detach() for param in group['params']] for group in self.param_groups]\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group, slow_Weight in zip(self.param_groups, self.slow_weights):\n",
    "            group['k_counter'] += 1\n",
    "            if group['k_counter'] == self.k:\n",
    "                for param, weight in zip(group['params'], slow_Weight):\n",
    "                    weight.data.add_(self.alpha, (param.data - weight.data))\n",
    "                    param.data.copy_(weight.data)\n",
    "                group['k_counter'] = 0\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_dict = self.optimizer.state_dict()\n",
    "        fast_state = fast_dict['state']\n",
    "        param_groups = fast_dict['param_groups']\n",
    "        slow_state = {(id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "                        for k, v in self.state.items()}\n",
    "        return {\n",
    "            'fast_state': fast_state,\n",
    "            'param_groups': param_groups,\n",
    "            'slow_state': slow_state\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_dict = {\n",
    "            'state': state_dict['fast_state'],\n",
    "            'param_groups': state_dict['param_groups']\n",
    "        }\n",
    "        slow_dict = {\n",
    "            'state': state_dict['slow_state'],\n",
    "            'param_groups': state_dict['param_groups']\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_dict)\n",
    "        self.optimizer.load_state_dict(fast_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5358b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = 0.0\n",
    "test_loss = 0.0\n",
    "best_acc = 0.0\n",
    "trn_losses = []\n",
    "trn_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "total = 0\n",
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8002987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, lr):\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbd95fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_find(clr, model, optimizer=None):\n",
    "\n",
    "    t = tqdm.tqdm(trainloader, leave=False, total=len(trainloader))\n",
    "    running_loss = 0.\n",
    "    avg_beta = 0.98\n",
    "    model.train()\n",
    "    \n",
    "    for i,data in enumerate(t):\n",
    "        input = data[0]\n",
    "        target = data[1]\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        var_ip, var_tg = Variable(input), Variable(target)\n",
    "        output = model(var_ip)\n",
    "        loss = criterion(output, var_tg)\n",
    "    \n",
    "        running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()\n",
    "        smoothed_loss = running_loss / (1 - avg_beta**(i+1))\n",
    "        t.set_postfix(loss=smoothed_loss)\n",
    "    \n",
    "        lr = clr.calc_lr(smoothed_loss)\n",
    "        if lr == -1 :\n",
    "            break\n",
    "        update_lr(optimizer, lr)   \n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    trainloader.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a56d6e",
   "metadata": {},
   "source": [
    "## Defining train and test function \n",
    "To train the model, you have to loop over our data iterator, feed the inputs to the network, and optimize.Then we are testing the model with batch of images from our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "126dc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=0, model=None, optimizer=None):\n",
    "    model.train()\n",
    "    global best_acc\n",
    "    global trn_accs, trn_losses\n",
    "    is_improving = True\n",
    "    counter = 0\n",
    "    running_loss = 0.\n",
    "    avg_beta = 0.98\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        input = data[0]\n",
    "        target = data[1]\n",
    "        bt_start = time.time()\n",
    "        var_ip, var_tg = Variable(input), Variable(target)\n",
    "                                    \n",
    "        output = model(var_ip)\n",
    "        loss = criterion(output, var_tg)\n",
    "        running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()\n",
    "        smoothed_loss = running_loss / (1 - avg_beta**(i+1))\n",
    "        trn_losses.append(smoothed_loss)\n",
    "            \n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output.data, target)\n",
    "        trn_accs.append(prec)\n",
    "        train_stats.append(smoothed_loss, prec, time.time()-bt_start)\n",
    "        if prec > best_acc :\n",
    "            best_acc = prec\n",
    "            save_checkpoint(model, True)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    trainloader.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bb16a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model=None):\n",
    "    # print(\"in test fn \")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        global val_accs, val_losses\n",
    "        running_loss = 0.\n",
    "        avg_beta = 0.98\n",
    "        for i, data in enumerate(validloader):\n",
    "            input =data[0]\n",
    "            target =data[1]\n",
    "            bt_start = time.time()\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            var_ip, var_tg = Variable(input), Variable(target)\n",
    "            output = model(var_ip)\n",
    "            loss = criterion(output, var_tg)\n",
    "            running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()\n",
    "            smoothed_loss = running_loss / (1 - avg_beta**(i+1))\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output.data, target, is_test=True)\n",
    "            test_stats.append(loss.item(), prec, time.time()-bt_start)\n",
    "            val_losses.append(smoothed_loss)\n",
    "            val_accs.append(prec)\n",
    "        validloader.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c3334cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model=None, sched=None, optimizer=None):\n",
    "    print(\"Epoch\\tTrn_loss\\tVal_loss\\tTrn_acc\\t\\tVal_acc\")\n",
    "    for j in range(epoch):\n",
    "        train(epoch=j, model=model, optimizer=optimizer)\n",
    "        \n",
    "        test(model)\n",
    "        if sched:\n",
    "            sched.step(j)\n",
    "        print(\"{}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\"\n",
    "              .format(j+1, trn_losses[-1], val_losses[-1], trn_accs[-1], val_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b676d359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision-0.13.0a0+f5afae5-py3.7-linux-x86_64.egg/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated, please use '{weights_param}' instead.\"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision-0.13.0a0+f5afae5-py3.7-linux-x86_64.egg/torchvision/models/_utils.py:220: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:1403: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  \" and \".join(warn_msg) + \" are deprecated. nn.Module.state_dict will not accept them in the future. \"\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=102)\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.require_grad = False\n",
    "    \n",
    "for param in model.fc.parameters():\n",
    "    param.require_grad = True\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "save_checkpoint(model, True, 'before_start_resnet18.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bca9db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = Lookahead(optim)\n",
    "\n",
    "clr = CLR(optim, len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "886ae8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, 'before_start_resnet18.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d93bf209",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "epoch = 30\n",
    "train_stats = AvgStats()\n",
    "test_stats = AvgStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92e32e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c43680",
   "metadata": {},
   "source": [
    "### Define a Loss function and optimizer\n",
    "Here we are using Classification Cross-Entropy loss and SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06585899",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = Lookahead(optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b70407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrn_loss\tVal_loss\tTrn_acc\t\tVal_acc\n",
      "1\t4.51921274\t4.50739813\t5.46875000\t4.68750000\n",
      "2\t3.38026991\t3.72153687\t10.93750000\t11.71875000\n",
      "3\t2.50118607\t3.00141191\t20.42410660\t21.04166603\n",
      "4\t1.76716530\t2.36381149\t30.75657845\t31.56250191\n",
      "5\t1.19526467\t1.90812218\t40.23437500\t40.62500000\n",
      "6\t0.84180764\t1.68413901\t47.52155304\t47.76041794\n",
      "7\t0.50074009\t1.53073525\t53.40073776\t53.79463959\n",
      "8\t0.37289218\t1.66946995\t58.05288696\t58.00781250\n",
      "9\t0.31988244\t1.43273461\t61.50568008\t61.56250381\n",
      "10\t0.24609586\t1.27182925\t64.41326904\t64.46875000\n",
      "11\t0.21823182\t1.18444765\t66.89814758\t67.07386017\n",
      "12\t0.17273188\t1.12508965\t69.14724731\t69.14062500\n",
      "13\t0.13504326\t1.21752918\t70.92285156\t70.88942719\n",
      "14\t0.10898909\t1.01823831\t72.48641205\t72.47768402\n",
      "15\t0.10730325\t1.04731810\t73.88091278\t73.87500000\n",
      "16\t0.10359325\t1.17327511\t75.07911682\t75.03906250\n",
      "17\t0.06230365\t1.26794255\t76.19047546\t76.17646790\n",
      "18\t0.11769052\t1.07591093\t77.14185333\t77.04861450\n",
      "19\t0.08496794\t1.07601881\t77.99201965\t77.94407654\n",
      "20\t0.12235398\t0.98256910\t78.77209473\t78.75000000\n",
      "21\t0.07408638\t1.16028357\t79.52223206\t79.47916412\n",
      "22\t0.09264925\t1.23339450\t80.14621735\t80.05681610\n",
      "23\t0.06895180\t1.09239197\t80.74287415\t80.69293213\n",
      "24\t0.06465927\t1.04041862\t81.34191132\t81.28906250\n",
      "25\t0.07464551\t1.03840292\t81.86743927\t81.80000305\n",
      "26\t0.05231631\t1.26247513\t82.34011841\t82.21153259\n",
      "27\t0.08767193\t1.30534637\t82.69589233\t82.53472137\n",
      "28\t0.05360390\t1.29636502\t83.01483917\t82.91294861\n"
     ]
    }
   ],
   "source": [
    "fit(model=model, optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(\"Total_time \",end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f21436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b6188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c5764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6275984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc84023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f134075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f17d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb850be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
